{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the csv as a pandas dataframe \n",
    "df = pd.read_csv(\"devanagari_df.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Defines the Feed Forward Neural Network Module.\n",
    "    This module takes in the number of hidden layers, the nodes per layer, the training data, and the activation function,\n",
    "        and outputs the model to be trained using its forward method.\n",
    "    Args:\n",
    "        num_hidden_layers (Int): the number of hidden layers of the neural network (doesn't include the input/output layers)\n",
    "        nodes_per_layer (List[Int]): the number of nodes per layer. The last one should be 1, and the length should be (2 + num_hidden_layers)\n",
    "        X_trn (Tensor([Double, Double])): a tensor representing the training data for the model\n",
    "        activation_function (Function): Must be one of torch.nn.Sigmoid(), torch.nn.ReLU(), torch.tanh(), torch.nn.Identity()\n",
    "    Returns:\n",
    "        The Feed Forward Neural Network model to be trained\n",
    "\"\"\"\n",
    "class Feed_Forward(torch.nn.Module):\n",
    "    def __init__(self, num_hidden_layers, nodes_per_layer, activation_function):\n",
    "        super(Feed_Forward, self).__init__()\n",
    "        # assert Module inputs adhere to constraints\n",
    "        assert(num_hidden_layers == len(nodes_per_layer) - 2)\n",
    "\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.nodes_per_layer = nodes_per_layer\n",
    "        self.activation_function = activation_function\n",
    "        self.layers = []\n",
    "        # define the first layer as going from the initial data count to the first hidden layer's node count\n",
    "        self.fc1 = torch.nn.Linear(nodes_per_layer[0], nodes_per_layer[1])\n",
    "        # Append the first layer and activation function to this Module\n",
    "        self.layers.append(self.fc1)\n",
    "        self.layers.append(activation_function)\n",
    "\n",
    "        # Append layers to the model for each hidden layer\n",
    "        for i in range(num_hidden_layers - 1):\n",
    "            self.layers.append(torch.nn.Linear(self.nodes_per_layer[i + 1], self.nodes_per_layer[i + 2]))\n",
    "            self.layers.append(activation_function)\n",
    "\n",
    "        # Append the output layer to the model\n",
    "        self.output_layer = torch.nn.Linear(self.nodes_per_layer[-2], self.nodes_per_layer[-1])\n",
    "        self.output_activation_sigmoid = torch.nn.Sigmoid()\n",
    "        self.layers.append(self.output_layer)\n",
    "        self.layers.append(self.output_activation_sigmoid)\n",
    "\n",
    "    # runs a step through the model, generating the output from this epoch of training\n",
    "    def forward(self, x):\n",
    "        model = torch.nn.Sequential(*self.layers)\n",
    "        return model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs, X_trn, Y_trn):\n",
    "    \"\"\"\n",
    "        Trains the given model, performing the given number of epochs over the given Training and Testing data.\n",
    "        Args:\n",
    "            model (torch.nn.Module): The Feed Forward Neural Network to be trained\n",
    "            epochs (int): The total number of epochs to train over\n",
    "            X_trn (Tensor([Double, Double])): the training features\n",
    "            Y_trn (Tensor(Int)): the training target\n",
    "    \"\"\"\n",
    "    # define a loss criteria for the model (use Cross-Entropy Loss, as this is a classification problem)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    # define an optimizer for the model (use Stochastic Gradient Descent)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "    # set the model to train\n",
    "    model.train()\n",
    "    # set default value for final layer output\n",
    "    final_layer_output = 0\n",
    "    # for every epoch, train the model\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        # Move forward through the model\n",
    "        y_pred = model(X_trn)\n",
    "        #y_pred = y_pred.type(torch.LongTensor)\n",
    "\n",
    "        # if this is the last epoch, we save the predictions\n",
    "        if(epoch == epochs - 1):\n",
    "            final_layer_output = y_pred.round()\n",
    "        # Compute the loss\n",
    "        loss = criterion(y_pred, Y_train)\n",
    "        # Back-propagate to adjust weights\n",
    "        loss.backward()\n",
    "        # Step through model\n",
    "        optimizer.step()\n",
    "    return model, final_layer_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_train_ff_nn(num_hidden_layers, nodes_per_layer, X_trn, Y_trn, activation_function, epochs):\n",
    "    \"\"\"\n",
    "        Creates a Feed Forward Neural Network, and then trains it over the specified number of epochs\n",
    "        Args:\n",
    "            num_hidden_layers (Int): the total number of hidden layers for the neural network, either 1 or 2\n",
    "            nodes_per_layer (List[Int]): the number of nodes for each layer of the neural network\n",
    "            dataset (List[(Double, Double), Int]): the training dataset for the model\n",
    "            activation_function (Function): the activation function used\n",
    "            epochs (Int): the number of epochs to train over\n",
    "        Returns:\n",
    "            the trained model\n",
    "\n",
    "    \"\"\"\n",
    "    # define the feed forward model\n",
    "    model = Feed_Forward(num_hidden_layers, nodes_per_layer, activation_function)\n",
    "    # train the model and get the final output\n",
    "    model, final_layer_output = train_model(model, epochs, X_trn, Y_trn)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, X_trn, Y_trn, X_tst, Y_tst):\n",
    "    # testing, so don't want to use a gradient\n",
    "    with torch.no_grad():\n",
    "        # testing the model\n",
    "        model.eval()\n",
    "        # make the predictions\n",
    "        y_pred = model(X_trn)\n",
    "        y_pred = torch.argmax(y_pred, dim=-1).cpu().detach().numpy()\n",
    "        # find the accuracy for training and testing\n",
    "        training_accuracy = accuracy_score(Y_trn, y_pred)\n",
    "        y_pred = model(X_tst)\n",
    "        y_pred = torch.argmax(y_pred, dim=-1).cpu().detach().numpy()\n",
    "        testing_accuracy = accuracy_score(Y_tst, y_pred)\n",
    "        return training_accuracy, testing_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.iloc[:, :-1]\n",
    "target = df.iloc[:, -1]\n",
    "target = [str(x) for x in target]\n",
    "label_encoder = LabelEncoder()\n",
    "target = label_encoder.fit_transform(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(features, target)\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "# turn the training and testing data into tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "Y_train = torch.LongTensor(Y_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "Y_test = torch.LongTensor(Y_test)f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(neural_network, features, target, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1, 2, 2, 3, 4]"
      ]
     },
     "metadata": {},
     "execution_count": 184
    }
   ],
   "source": [
    "nodes_per_layer_list = [[X_train.shape[1], 100, 46], [X_train.shape[1], 100, 100, 46], [X_train.shape[1], 500, 100, 46], [X_train.shape[1], 100, 100, 100, 46], [X_train.shape[1], 100, 100, 100, 100, 46]]\n",
    "hidden_layers_list  = [len(x) - 2 for x in nodes_per_layer_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running model with  1 hidden layers:  [1024, 100, 46]\n",
      "\tAverage training accuracy:  0.6660398237541078\n",
      "\tAverage testing accuracy:  0.6660340108037875 \n",
      "\n",
      "Running model with  2 hidden layers:  [1024, 100, 100, 46]\n",
      "\tAverage training accuracy:  0.5559075185532732\n",
      "\tAverage testing accuracy:  0.5559032428013941 \n",
      "\n",
      "Running model with  2 hidden layers:  [1024, 500, 100, 46]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-233-bb37befb4613>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#for a in activation_functions_list:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_and_train_ff_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes_per_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mcurr_training_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_testing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtraining_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_count\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcurr_training_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-117-97561fb54387>\u001b[0m in \u001b[0;36mcreate_and_train_ff_nn\u001b[0;34m(num_hidden_layers, nodes_per_layer, X_trn, Y_trn, activation_function, epochs)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeed_Forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes_per_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# train the model and get the final output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_layer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_trn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-178-fad415cb8ffc>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, epochs, X_trn, Y_trn)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Back-propagate to adjust weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Step through model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "a = torch.nn.Sigmoid()\n",
    "n_splits = 5\n",
    "#activation_functions_list = [torch.nn.ReLU(), torch.nn.Sigmoid()]\n",
    "kf = StratifiedKFold(n_splits=n_splits, random_state=4400, shuffle=True)\n",
    "for i, nodes_per_layer in enumerate(nodes_per_layer_list):\n",
    "    num_hidden_layers = hidden_layers_list[i]\n",
    "    training_count = 0.0\n",
    "    testing_count = 0.0\n",
    "    print(\"Running model with \", num_hidden_layers, \"hidden layers: \", nodes_per_layer)\n",
    "    for train_index, test_index in kf.split(features, target):\n",
    "        X_train, X_test = features[train_index[0]:train_index[-1]], features[test_index[0]:test_index[-1]]\n",
    "        Y_train, Y_test = target[train_index[0]:train_index[-1]], target[test_index[0]:test_index[-1]]\n",
    "        X_train = X_train.to_numpy()\n",
    "        X_test = X_test.to_numpy()\n",
    "        # turn the training and testing data into tensors\n",
    "        X_train = torch.FloatTensor(X_train)\n",
    "        Y_train = torch.LongTensor(Y_train)\n",
    "        X_test = torch.FloatTensor(X_test)\n",
    "        Y_test = torch.LongTensor(Y_test)\n",
    "\n",
    "        #for a in activation_functions_list:\n",
    "        model = create_and_train_ff_nn(num_hidden_layers, nodes_per_layer, X_train, Y_train, a, epochs)\n",
    "        curr_training_count, curr_testing_count = test_model(model, X_train, Y_train, X_test, Y_test)\n",
    "        training_count = training_count + curr_training_count\n",
    "        testing_count = testing_count + curr_testing_count\n",
    "    training_accuracy = training_count / n_splits\n",
    "    testing_accuracy = testing_count / n_splits\n",
    "    print(\"\\tAverage training accuracy: \", training_accuracy)\n",
    "    print(\"\\tAverage testing accuracy: \", testing_accuracy, \"\\n\")"
   ]
  },
  {
   "source": [
    "ReLU:\n",
    "Running model with  1 hidden layers:  [1024, 100, 46]\n",
    "\tTraining accuracy:  0.5156811594202898\n",
    "\tTesting accuracy:  0.5025652173913043\n",
    "Running model with  2 hidden layers:  [1024, 100, 100, 46]\n",
    "\tTraining accuracy:  0.10592753623188406\n",
    "\tTesting accuracy:  0.104\n",
    "Running model with  2 hidden layers:  [1024, 500, 100, 46]\n",
    "\tTraining accuracy:  0.07572463768115942\n",
    "\tTesting accuracy:  0.07152173913043479\n",
    "Running model with  3 hidden layers:  [1024, 100, 100, 100, 46]\n",
    "\tTraining accuracy:  0.28982608695652173\n",
    "\tTesting accuracy:  0.2658695652173913\n",
    "Running model with  4 hidden layers:  [1024, 100, 100, 100, 100, 46]\n",
    "\tTraining accuracy:  0.36144927536231886\n",
    "\tTesting accuracy:  0.34991304347826085"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigmoid:\n",
    "Running model with  1 hidden layers:  [1024, 100, 46]\n",
    "\tTraining accuracy:  0.7673188405797101\n",
    "\tTesting accuracy:  0.7322608695652174\n",
    "Running model with  2 hidden layers:  [1024, 100, 100, 46]\n",
    "\tTraining accuracy:  0.6775072463768116\n",
    "\tTesting accuracy:  0.6507391304347826\n",
    "Running model with  2 hidden layers:  [1024, 500, 100, 46]\n",
    "\tTraining accuracy:  0.802695652173913\n",
    "\tTesting accuracy:  0.759695652173913\n",
    "Running model with  3 hidden layers:  [1024, 100, 100, 100, 46]\n",
    "\tTraining accuracy:  0.5211159420289855\n",
    "\tTesting accuracy:  0.5023478260869565\n",
    "Running model with  4 hidden layers:  [1024, 100, 100, 100, 100, 46]\n",
    "\tTraining accuracy:  0.25714492753623186\n",
    "\tTesting accuracy:  0.24178260869565218"
   ]
  }
 ]
}